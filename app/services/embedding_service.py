from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Optional
import logging

logger = logging.getLogger(__name__)

class EmbeddingService:
    """í…ìŠ¤íŠ¸ ìž„ë² ë”© ìƒì„± ë° ìœ ì‚¬ë„ ê³„ì‚° ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        # í•œêµ­ì–´ ìµœì í™” ëª¨ë¸ (ê²½ëŸ‰í™”ëœ ë²„ì „)
        self.model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        logger.info("ðŸ¤– ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def generate_embedding(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ì˜ ìž„ë² ë”© ë²¡í„° ìƒì„±"""
        if not text or not text.strip():
            return [0.0] * 768
            
        try:
            clean_text = self._preprocess_text(text)
            embedding = self.model.encode(clean_text)
            return embedding.tolist()
            
        except Exception as e:
            logger.error(f"ìž„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return [0.0] * 768
    
    def generate_batch_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ë°°ì¹˜ ìž„ë² ë”© ìƒì„± (ì„±ëŠ¥ ìµœì í™”)"""
        if not texts:
            return []
            
        try:
            clean_texts = [self._preprocess_text(text) for text in texts]
            embeddings = self.model.encode(clean_texts, batch_size=32)
            return [emb.tolist() for emb in embeddings]
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ìž„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return [[0.0] * 768 for _ in texts]
    
    def calculate_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """ë‘ ìž„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)
            
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
                
            return float(dot_product / (norm1 * norm2))
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _preprocess_text(self, text: str) -> str:
        """ìž„ë² ë”© ìƒì„± ì „ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
            
        text = text.strip()
        text = text.replace('\n', ' ').replace('\r', ' ')
        text = ' '.join(text.split())
        
        # í† í° ê¸¸ì´ ì œí•œ (ëª¨ë¸ ì œì•½)
        if len(text) > 1000:
            text = text[:1000]
            
        return text

embedding_service = EmbeddingService()
